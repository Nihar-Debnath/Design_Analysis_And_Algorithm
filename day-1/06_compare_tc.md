## ğŸ§© What is **Time Complexity**?

**Time complexity** measures how the **execution time** (or number of operations) of an algorithm **increases** with the size of input ( n ).

It tells us *how efficiently* an algorithm performs when the input grows larger.

---

## âš™ï¸ **Different Types of Time Complexities**

Here are the most common **time complexity classes**, starting from the **fastest (best)** to the **slowest (worst)**:

| Order | Complexity                       | Example Algorithm                    | Growth Rate Description                            |
| :---: | :------------------------------- | :----------------------------------- | :------------------------------------------------- |
|  1ï¸âƒ£  | **O(1)** â€” Constant Time         | Accessing an element in an array     | Always takes same time, independent of input size  |
|  2ï¸âƒ£  | **O(log n)** â€” Logarithmic Time  | Binary Search                        | Time grows slowly even if input increases a lot    |
|  3ï¸âƒ£  | **O(n)** â€” Linear Time           | Linear Search, Traversing an array   | Time increases directly with input size            |
|  4ï¸âƒ£  | **O(n log n)** â€” Log-Linear Time | Merge Sort, Quick Sort (avg case)    | Slightly slower than linear, faster than quadratic |
|  5ï¸âƒ£  | **O(nÂ²)** â€” Quadratic Time       | Bubble Sort, Insertion Sort          | Time increases rapidly with nÂ²                     |
|  6ï¸âƒ£  | **O(nÂ³)** â€” Cubic Time           | Matrix Multiplication (naive)        | Even more rapid growth                             |
|  7ï¸âƒ£  | **O(2â¿)** â€” Exponential Time     | Recursive Fibonacci, Subset problems | Grows extremely fast â€” impractical for large n     |
|  8ï¸âƒ£  | **O(n!)** â€” Factorial Time       | Travelling Salesman (Brute Force)    | Grows the fastest â€” only works for tiny inputs     |

---

## ğŸ“ˆ **Comparison: Increasing Order of Growth**

Hereâ€™s how they are **arranged from smallest (fastest)** to **largest (slowest)**:

[
O(1) \ < \ O(\log n) \ < \ O(n) \ < \ O(n \log n) \ < \ O(n^2) \ < \ O(n^3) \ < \ O(2^n) \ < \ O(n!)
]

---

## ğŸ§® **Intuitive Growth Example**

Letâ€™s take ( n = 10 ):

| Complexity | Approx. Operations |
| ---------- | ------------------ |
| O(1)       | 1                  |
| O(log n)   | ~3                 |
| O(n)       | 10                 |
| O(n log n) | ~33                |
| O(nÂ²)      | 100                |
| O(nÂ³)      | 1000               |
| O(2â¿)      | 1024               |
| O(n!)      | 3,628,800 ğŸ˜±       |

â¡ï¸ You can see how **exponential and factorial** time complexities explode in growth!

---

## ğŸ§  **Summary (In Words)**

* **Constant (O(1))** â†’ Fastest
* **Logarithmic (O(log n))** â†’ Very efficient (e.g. binary search)
* **Linear (O(n))** â†’ Grows directly with input
* **Linearithmic (O(n log n))** â†’ Common in efficient sorting algorithms
* **Quadratic (O(nÂ²))** â†’ Typical of nested loops
* **Cubic (O(nÂ³))** â†’ Three nested loops
* **Exponential (O(2â¿))** â†’ Very expensive (recursion-heavy problems)
* **Factorial (O(n!))** â†’ Almost impossible for large n

---

## ğŸ“Š Visual Analogy (How They Grow)

If you plotted them on a graph for large ( n ):

```
|                             O(n!)
|                        O(2^n)
|                O(n^3)
|            O(n^2)
|        O(n log n)
|      O(n)
|   O(log n)
| O(1)
+---------------------------------> n
```
