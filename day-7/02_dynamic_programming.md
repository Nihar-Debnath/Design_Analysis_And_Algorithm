# 1ï¸âƒ£ Why Do We Need Dynamic Programming?

### The core problem:

Some problems have:

* **Huge number of overlapping subproblems**
* **Recursive solutions that repeat work**

Example:

* Fibonacci numbers
* Shortest path problems
* Knapsack
* Matrix chain multiplication

ðŸ‘‰ **Brute force / recursion becomes very slow**

---

# 2ï¸âƒ£ What is Dynamic Programming (DP)?

## ðŸ“˜ Definition (Exam-ready)

> **Dynamic Programming** is an algorithmic technique that solves complex problems by:
>
> * Breaking them into **smaller overlapping subproblems**
> * Solving each subproblem **once**
> * Storing the result and reusing it

ðŸ“Œ Coined by **Richard Bellman**

---

# 3ï¸âƒ£ Two Core Properties of DP (VERY IMPORTANT)

A problem can be solved using DP **only if both are true**:

---

## âœ… 1. Optimal Substructure

> Optimal solution of the problem can be built from optimal solutions of its subproblems.

Example:

* Shortest path A â†’ D depends on shortest path A â†’ B and B â†’ D

---

## âœ… 2. Overlapping Subproblems

> Same subproblems are solved multiple times.

Example:

* Fibonacci(5) calculates Fibonacci(3) many times

---

# 4ï¸âƒ£ Simple Real-World Intuition for DP

### Problem:

You want to climb stairs with **minimum cost**.

âŒ Without DP:

* You recalculate same steps again and again

âœ… With DP:

* Store cost at each step
* Reuse it

ðŸ‘‰ **Work once, remember forever**

---

# 5ï¸âƒ£ How DP Works (Two Approaches)

---

## ðŸ”¹ 1. Top-Down (Memoization)

* Start from main problem
* Go down recursively
* Store answers in a table

ðŸ§  Think: **â€œSolve â†’ store â†’ reuseâ€**

---

## ðŸ”¹ 2. Bottom-Up (Tabulation)

* Solve smallest subproblem first
* Build up to final answer

ðŸ§  Think: **â€œBuild step by stepâ€**

---

# 6ï¸âƒ£ What is a Greedy Algorithm?

## ðŸ“˜ Definition

> A **Greedy Algorithm** makes the **best local choice** at each step, hoping it leads to the global optimum.

ðŸ“Œ It **never reconsiders** its decision.

---

## ðŸ§  Real-Life Greedy Example

### Problem:

Give change using minimum coins.

Coins: `1, 5, 10`

Greedy choice:

* Always pick **largest coin first**

For 28:

```
10 + 10 + 5 + 1 + 1 + 1
```

Works âœ”

---

# 7ï¸âƒ£ Why Greedy Sometimes FAILS?

Coins: `1, 3, 4`
Amount: `6`

### Greedy:

```
4 + 1 + 1 = 3 coins
```

### Optimal:

```
3 + 3 = 2 coins
```

âŒ Greedy fails
âœ… DP finds optimal

---

# 8ï¸âƒ£ Greedy vs Dynamic Programming (EXAM GOLD)

| Feature          | Greedy          | Dynamic Programming |
| ---------------- | --------------- | ------------------- |
| Decision         | Local optimum   | Global optimum      |
| Backtracking     | âŒ No            | âœ… Yes               |
| Memory           | Very low        | Higher              |
| Complexity       | Fast            | Slower              |
| Accuracy         | Sometimes wrong | Always correct      |
| Reuse of results | âŒ No            | âœ… Yes               |

---

# 9ï¸âƒ£ When to Use Greedy vs DP?

### Use **Greedy** when:

âœ” Problem has **greedy-choice property**
âœ” Local choice leads to global optimum
âœ” Speed is priority

Examples:

* Kruskalâ€™s Algorithm
* Primâ€™s Algorithm
* Huffman Coding
* Activity Selection

---

### Use **Dynamic Programming** when:

âœ” Overlapping subproblems
âœ” Optimal substructure
âœ” Greedy fails

Examples:

* Knapsack (0/1)
* Longest Common Subsequence
* Matrix Chain Multiplication
* Fibonacci
* Coin Change (general)

---

# ðŸ”Ÿ DAA Exam One-Line Definitions

### Dynamic Programming:

> Solve once, store, reuse.

### Greedy:

> Choose best now, hope for best later.

---

# ðŸ§  Memory Trick

* **Greedy** â†’ *Fast but risky*
* **DP** â†’ *Slow but safe*






---
---
---
---
---




# ðŸ“˜ Introduction to Dynamic Programming (DP)

**Dynamic Programming | Greedy vs Dynamic Programming | DAA**

---

## 1ï¸âƒ£ What Problem Does Dynamic Programming Solve?

Many algorithmic problems involve:

* Multiple **choices at each stage**
* Need to find the **best (optimal) solution**
* Repeated solving of the **same subproblems**

âŒ Brute force or naive recursion:

* Tries all possibilities
* Very slow (exponential time)

âœ… **Dynamic Programming**:

* Avoids repeated work
* Guarantees **optimal solution**

---

## 2ï¸âƒ£ Greedy Method vs Dynamic Programming

---

### ðŸ”¹ Greedy Method

**Definition**

> Greedy method makes the **locally optimal choice** at each step, hoping it leads to the global optimum.

**Key Idea**
ðŸ‘‰ â€œTake the best option available right now.â€

**Important Limitation**
âŒ Local optimum â‰  Global optimum (always)

---

### âŒ Greedy Failure (Shortest Path Example)

Suppose:

* At a node, greedy chooses the **smallest immediate edge**
* Later, that choice leads to a **larger total path cost**

âœ” Greedy does **not explore all possibilities**
âœ” Once a decision is made, it is **never changed**

---

### ðŸ”¹ Dynamic Programming

**Definition (Exam-ready)**

> Dynamic Programming solves a problem by exploring **all possible sequences of decisions**, breaking the problem into **overlapping subproblems**, solving each **once**, and storing the results.

âœ” Guarantees **global optimal solution**
âœ” Trades **extra memory** for **correctness and efficiency**

---

## 3ï¸âƒ£ Why Dynamic Programming Works (Core Difference)

| Greedy         | Dynamic Programming            |
| -------------- | ------------------------------ |
| Local decision | Global decision                |
| Fast           | Slightly slower                |
| Low memory     | Uses table (memory)            |
| May fail       | Always correct (if applicable) |

---

## 4ï¸âƒ£ Two Key Features of Dynamic Programming (MOST IMPORTANT)

A problem can be solved using DP **only if BOTH conditions are satisfied**.

---

## âœ… 1. Optimal Substructure

**Meaning**

> The optimal solution of a problem can be constructed from optimal solutions of its subproblems.

### Example:

Shortest path A â†’ D depends on:

* Shortest path A â†’ B
* Shortest path B â†’ D

ðŸ“Œ Similar to **Divide and Conquer**

* Example: Merge Sort
* But **this alone is not enough for DP**

---

## âœ… 2. Overlapping Subproblems (CRUCIAL DIFFERENCE)

**Meaning**

> Same subproblems are solved **again and again**.

ðŸ“Œ This is what separates **Dynamic Programming** from **Divide and Conquer**.

---

## 5ï¸âƒ£ Fibonacci Series Example (Classic DP Explanation)

### Recursive Definition:

[
F(n) = F(n-1) + F(n-2)
]

---

### âŒ Without DP (Naive Recursion)

For `F(4)`:

* `F(3)` and `F(2)` are recalculated multiple times
* `F(2)`, `F(1)`, `F(0)` repeat again and again

â± Time Complexity:
[
O(2^n) \quad \text{(Exponential)}
]

---

### âœ… With DP (Memoization / Tabulation)

* Store results of `F(0), F(1), F(2), ...`
* Each Fibonacci value is computed **only once**

â± Time Complexity:
[
O(n)
]

ðŸ§  **Core DP idea**:

> â€œSolve once â†’ Store â†’ Reuseâ€

---

## 6ï¸âƒ£ How Dynamic Programming Improves Efficiency

| Approach            | Time Complexity     |
| ------------------- | ------------------- |
| Naive recursion     | Exponential         |
| Dynamic Programming | Polynomial / Linear |

DP avoids recomputation by using:

* **Memoization (Top-Down)**
* **Tabulation (Bottom-Up)**

---

## 7ï¸âƒ£ Applications of Dynamic Programming (From Video)

Dynamic Programming is widely used in **optimization problems**:

1. **Matrix Chain Multiplication**
2. **Multi-Stage Graph**
3. **Travelling Salesman Problem (TSP)**
4. **Longest Common Subsequence (LCS)**
5. **Subset Sum Problem**
6. **All-Pairs Shortest Path (Floyd-Warshall)**
7. **0-1 Knapsack Problem**

---

## 8ï¸âƒ£ One-Line Exam Answers (Very Important)

* **Greedy Algorithm**:

  > Makes locally optimal choices; may not give global optimum.

* **Dynamic Programming**:

  > Solves overlapping subproblems and stores results to ensure optimal solution.

* **Key Difference from Divide & Conquer**:

  > DP has **overlapping subproblems**.

---

## 9ï¸âƒ£ Memory Trick (Never Forget)

* **Greedy** â†’ *Fast but risky*
* **Dynamic Programming** â†’ *Slow but safe*

---

## ðŸ”Ÿ Final Summary (Perfect Conclusion Line)

> Dynamic Programming guarantees optimal solutions by solving and storing overlapping subproblems, whereas the Greedy method makes quick local decisions that may fail to produce a global optimum.
