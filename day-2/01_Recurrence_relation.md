## What is a recurrence relation? (Beginner version)

### Step 1: Forget algorithms for a moment

Think about this **simple idea**:

> Sometimes, a value depends on **its previous value**.

Example from daily life:

* Your **age next year** depends on your **age this year**.
* Your **bank balance** tomorrow depends on today‚Äôs balance.

This kind of ‚Äúself-dependence‚Äù is the core idea.

---

## Step 2: Simple math example (not DAA yet)

Consider this rule:

> ‚ÄúEach number is double the previous one‚Äù

\[
f(n) = 2f(n-1)
\]

This is a **recurrence relation** because:

* The value at `n`
* Depends on the value at `n-1`

üëâ **Any formula that defines something using its smaller version is a recurrence relation.**

---

## Step 3: Now the definition (very simple)

> **A recurrence relation is an equation that defines a problem in terms of smaller instances of the same problem.**

Read that again slowly.

* ‚ÄúDefines a problem‚Äù
* ‚ÄúUsing smaller instances‚Äù
* ‚ÄúOf the same problem‚Äù

That‚Äôs it. No complexity yet.

---

## Step 4: Why ‚Äúrecurrence‚Äù?

The word **recurrence** comes from **‚Äúrecur‚Äù**, which means:

> **to happen again and again**

So:

* Same problem
* Solved again
* With smaller input

---

## Step 5: Extremely simple DAA-style example

Imagine this function:

```text
sum(n):
    if n == 1 return 1
    return n + sum(n-1)
```

Here:

* To compute `sum(n)`
* You need `sum(n-1)`

So we write:

\[
T(n) = T(n-1) + c
\]

This is a **recurrence relation**.

Why?

* `T(n)` depends on `T(n-1)`
* Same problem, smaller size

---

## Step 6: One sentence you should ALWAYS remember

> **A recurrence relation expresses a problem‚Äôs solution using solutions of smaller versions of the same problem.**

This sentence works:

* In exams
* In interviews
* In understanding algorithms

---

## Step 7: Why recurrence relations appear in DAA

In DAA, many algorithms are **recursive**:

* A function calls **itself**
* With **smaller input**

Whenever you see:

* recursion
  ‚Üí a recurrence relation exists.

---

## Step 8: What a recurrence relation is NOT

‚ùå It is **not** the final time complexity
‚ùå It is **not** Big-O notation
‚ùå It is **not** a loop
‚ùå It is **not** Master Theorem

It is just:

> **A relationship showing dependency on smaller inputs**

---

## Step 9: Ultra-short memory hook üß†

Remember this line forever:

> **‚ÄúSame problem, smaller input.‚Äù**

If that exists ‚Üí **recurrence relation exists**.

---
---
---
---
---


## üß† What is a Recurrence Relation?

A **recurrence relation** is simply an **equation that defines a problem in terms of smaller subproblems**.

In other words, it shows how the **time complexity** of a problem depends on the **time complexity of smaller versions** of the same problem.

---

### üîπ Example 1: Linear Search

If you check each element one by one in an array of size `n`:

* You check one element ‚Üí constant time = **O(1)**
* Then you search the remaining `n - 1` elements.

So,
üëâ **T(n) = T(n - 1) + O(1)**
(base case: when `n = 1`, T(1) = O(1))

That‚Äôs a **recurrence relation**.

It means:

> Time to solve size `n` = time to solve smaller problem (size `n-1`) + constant work.

---

## üîπ Example 2: Binary Search (our main focus)

Let‚Äôs recall how **binary search** works:

1. You have a **sorted array**.
2. Check the **middle element**:

   * If it‚Äôs the target ‚Üí done.
   * If target < middle ‚Üí search **left half**.
   * If target > middle ‚Üí search **right half**.

So, every time, the **problem size reduces by half**.

---

### üß© Step 1: Define What We‚Äôre Measuring

Let‚Äôs define
üëâ **T(n)** = time taken by binary search on an array of size `n`.

---

### üß© Step 2: Identify the Work Done at Each Step

At each step:

* Checking the middle element ‚Üí takes **constant time** = O(1)
* Then we search **only one half** of the array ‚Üí of size `n/2`

---

### üß© Step 3: Write the Recurrence Relation

So we get:

$$
T(n) = T(n/2) + O(1)
$$

and
$$
T(1) = O(1)
$$

That‚Äôs the **recurrence relation for binary search**.

It literally says:

> ‚ÄúTo search `n` elements, we only need to search `n/2` elements next time ‚Äî plus a small constant amount of work.‚Äù

---

## üßÆ Step 4: Solve the Recurrence

Now we need to find what **T(n)** equals in Big O notation.

We‚Äôll solve:

$$
T(n) = T(n/2) + 1
$$

(for simplicity, drop constants)

---

### ü™ú Method 1: Recursion Tree Method (Visualization)

Let‚Äôs ‚Äúunfold‚Äù the recurrence:

```
T(n) = T(n/2) + 1
     = T(n/4) + 1 + 1
     = T(n/8) + 1 + 1 + 1
     = ...
```

At each level:

* The problem size divides by 2.
* The number of levels = how many times you can divide n by 2 until you reach 1.

That is:
$$
\text{Levels} = \log_2 n
$$

And at each level, constant work `1` is done.

So total work =
$$
1 + 1 + 1 + ... + 1 \text{ (log n times)} = O(\log n)
$$

‚úÖ **Final Answer:**
$$
T(n) = O(\log n)
$$

---

### ü™ú Method 2: Master Theorem (Shortcut Method)

Recurrence:
$$
T(n) = aT(n/b) + f(n)
$$

where

* `a` = number of subproblems
* `b` = factor by which size reduces
* `f(n)` = work done outside recursion

For binary search:

* `a = 1` (one subproblem)
* `b = 2` (size divides by 2)
* `f(n) = O(1)`

Compute:
$$
n^{\log_b a} = n^{\log_2 1} = n^0 = 1
$$

Now compare `f(n)` with `n^{\log_b a}`:

* `f(n)` = O(1)
* `n^{\log_b a}` = 1

They are the same order ‚Üí Case 2 of Master Theorem:

$$
T(n) = O(\log n)
$$

‚úÖ Again, **T(n) = O(log n)**

---

## ‚úÖ Final Summary

| Concept                      | Explanation                                                    |
| ---------------------------- | -------------------------------------------------------------- |
| **Recurrence Relation**      | Equation expressing time for size `n` in terms of smaller size |
| **Binary Search Recurrence** | `T(n) = T(n/2) + O(1)`                                         |
| **Base Case**                | `T(1) = O(1)`                                                  |
| **Solving**                  | Using recursion tree or master theorem                         |
| **Final Complexity**         | `O(log n)`                                                     |

---

## üß© Quick Intuition

At each binary search step:

* The array becomes half smaller.
* The number of halvings until one element remains = **log‚ÇÇn**.

So the total time = **O(log n)**.


---

<br>
<br>
<br>

---

## üß© What does **T** mean?

When we write **T(n)** in algorithm analysis, the **T** stands for **Time** (or sometimes **Total Time**).

So:

> **T(n)** = the **total time** (or number of operations) your algorithm takes to handle an input of size **n**.

That‚Äôs all it means.

It‚Äôs just a **function name** ‚Äî we could‚Äôve called it anything, like `Time(n)` or `Cost(n)` or `F(n)` ‚Äî but in computer science, people usually write it as **T(n)** for "Time as a function of n."

---

### üß† Analogy: Think Like a Math Function

If you have a math function:

> f(x) = x¬≤ + 1
> that gives you a number for any x.

Similarly, in algorithms:

> T(n) = time it takes to solve input of size n.

So if n = 10, T(10) means ‚Äútime to run the algorithm on 10 elements.‚Äù

---

## üí° Where does T(n) come from?

When we analyze an algorithm, we want to **describe how its running time grows** when the input size increases.

We start by thinking:

* What‚Äôs the time for the **biggest** problem (size n)?
* How is it related to **smaller subproblems**?

That relation ‚Äî between big and small problems ‚Äî gives us a **recurrence relation**.

---

### Example 1: Binary Search Intuition

Suppose you‚Äôre searching a sorted array of size **n**.

Binary Search does:

1. Check the **middle** element ‚Üí constant time ‚Üí let‚Äôs call it **1 unit of time**.
2. Then search **half** the array ‚Üí so time = whatever time it takes to search n/2 elements.

That means:

> Total time = (time for smaller half) + (constant checking work)

We can express that as:

$$
T(n) = T(n/2) + 1
$$

That‚Äôs the **recurrence**:

* **T(n)** = time for the whole array
* **T(n/2)** = time for the smaller half
* **+ 1** = constant extra work

---

### Example 2: Linear Search Intuition

You check one element at a time:

$$
T(n) = T(n - 1) + 1
$$

* You checked one element ‚Üí +1 constant time
* You have n‚Äì1 elements left ‚Üí smaller problem

Again, **T(n)** just represents ‚Äútime for problem size n.‚Äù

---

## üîπ Why not just use O(n) directly?

Because recurrence relations help us **derive** the Big-O complexity.
We use them when an algorithm‚Äôs structure is **recursive**, i.e., it calls itself on smaller inputs.

Binary Search, Merge Sort, Quick Sort, etc. ‚Äî all these are recursive, so recurrence is the natural way to describe their time.

---

## ‚úÖ Summary

| Symbol      | Meaning                                          |
| ----------- | ------------------------------------------------ |
| **T(n)**    | Total time (or number of steps) for input size n |
| **T(n/2)**  | Time for a smaller subproblem (half the size)    |
| **+1 / +c** | Extra constant work outside recursion            |
| **T(1)**    | Base case (smallest input, takes constant time)  |

---

**So T(n) comes from ‚ÄúTime taken by algorithm for input size n.‚Äù**
We use it to express how that time depends on smaller instances of the same problem.
